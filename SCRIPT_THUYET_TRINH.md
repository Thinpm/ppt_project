# SCRIPT THUYáº¾T TRÃŒNH CODE - NEURAL NETWORK

## ğŸ¯ Má»¤C TIÃŠU THUYáº¾T TRÃŒNH

Tháº§y muá»‘n tháº¥y 3 Ä‘iá»u:
1. âœ… **TÃ­nh Ä‘Æ°á»£c gradient** - Biáº¿t rÃµ loss phá»¥ thuá»™c biáº¿n nÃ o, tÃ­nh gradient báº±ng chain rule
2. âœ… **Code cháº¡y Ä‘Æ°á»£c** - KhÃ´ng nÃ³i suÃ´ng, cÃ³ káº¿t quáº£ thá»±c táº¿
3. âœ… **Thiáº¿t káº¿ adaptive** - ThÃªm/bá»›t layer khÃ´ng cáº§n viáº¿t láº¡i code

---

## ğŸ“‹ Cáº¤U TRÃšC THUYáº¾T TRÃŒNH (10-15 phÃºt)

### PHáº¦N 1: GIá»šI THIá»†U (1 phÃºt)

**NÃ³i:**
> "Em xin chÃ o tháº§y. HÃ´m nay em sáº½ trÃ¬nh bÃ y vá» Neural Network tá»« Ä‘áº§u, táº­p trung vÃ o 3 Ä‘iá»ƒm chÃ­nh:
> - TÃ­nh gradient báº±ng Backpropagation tá»± implement
> - Code cháº¡y Ä‘Æ°á»£c vá»›i káº¿t quáº£ thá»±c táº¿
> - Thiáº¿t káº¿ adaptive, dá»… má»Ÿ rá»™ng"

**LÃ m:**
- Má»Ÿ terminal, cd vÃ o thÆ° má»¥c project
- Giá»›i thiá»‡u cáº¥u trÃºc project

---

### PHáº¦N 2: TÃNH GRADIENT - BACKPROPAGATION (4-5 phÃºt)

**ÄÃ¢y lÃ  pháº§n QUAN TRá»ŒNG NHáº¤T!**

#### 2.1. PhÃ¡t biá»ƒu bÃ i toÃ¡n tá»‘i Æ°u (1 phÃºt)

**NÃ³i:**
> "Äáº§u tiÃªn, em phÃ¡t biá»ƒu bÃ i toÃ¡n dÆ°á»›i dáº¡ng tá»‘i Æ°u.
> 
> Biáº¿n tá»‘i Æ°u lÃ  táº¥t cáº£ trá»ng sá»‘ vÃ  bias: Î˜ = {W^(1), b^(1), ..., W^(L), b^(L)}
> 
> HÃ m loss: L(Î˜) = 0.5 Ã— Î£(y_pred - y_true)Â²
> 
> BÃ i toÃ¡n: TÃ¬m Î˜ Ä‘á»ƒ L(Î˜) Ä‘áº¡t cá»±c tiá»ƒu."

**LÃ m:**
- Chá»‰ vÃ o cÃ´ng thá»©c trong bÃ¡o cÃ¡o/slide

#### 2.2. TÃ­nh gradient báº±ng Chain Rule (2-3 phÃºt)

**NÃ³i:**
> "Äá»ƒ tá»‘i Æ°u, em cáº§n tÃ­nh gradient. Em tÃ­nh gradient báº±ng Chain Rule, khÃ´ng dÃ¹ng automatic differentiation.
> 
> Gradient cá»§a loss theo output: Î´^(L) = y_pred - y_true
> 
> Lan truyá»n ngÆ°á»£c qua cÃ¡c layer:
> - Gradient cá»§a W: grad_W = Î´ @ a^T
> - Gradient cá»§a b: grad_b = Î´
> - Gradient truyá»n vá» layer trÆ°á»›c: grad_x = W^T @ Î´"

**LÃ m:**
- Má»Ÿ file `network/layers.py`
- Chá»‰ vÃ o hÃ m `backward()` cá»§a `DenseLayer`

**Code cáº§n chá»‰:**
```python
def backward(self, grad_out, lr):
    # Gradient cá»§a W: grad_W = grad_out @ x^T (theo chain rule)
    grad_W = np.outer(grad_out, self.x)
    
    # Gradient cá»§a b: grad_b = grad_out (theo chain rule)
    grad_b = grad_out
    
    # Gradient truyá»n vá» layer trÆ°á»›c: grad_x = W^T @ grad_out
    grad_x = self.W.T @ grad_out
    
    return grad_x
```

**NÃ³i:**
> "ÄÃ¢y lÃ  cÃ´ng thá»©c toÃ¡n há»c, em tá»± implement, khÃ´ng dÃ¹ng thÆ° viá»‡n AI nhÆ° PyTorch hay TensorFlow."

**LÃ m:**
- Má»Ÿ file `network/loss.py`
- Chá»‰ vÃ o hÃ m `backward()` cá»§a `MSELoss`

**Code cáº§n chá»‰:**
```python
def backward(self):
    # Äáº¡o hÃ m cá»§a 0.5*(y_pred - y_true)^2 theo y_pred = y_pred - y_true
    return self.y_pred - self.y_true
```

**NÃ³i:**
> "Gradient Ä‘Æ°á»£c tÃ­nh hoÃ n toÃ n báº±ng cÃ´ng thá»©c toÃ¡n há»c, khÃ´ng dÃ¹ng automatic differentiation."

---

### PHáº¦N 3: CODE CHáº Y ÄÆ¯á»¢C - DEMO (3-4 phÃºt)

#### 3.1. Cháº¡y demo chÃ­nh (2 phÃºt)

**NÃ³i:**
> "BÃ¢y giá» em sáº½ cháº¡y code Ä‘á»ƒ chá»©ng minh nÃ³ hoáº¡t Ä‘á»™ng."

**LÃ m:**
```bash
cd demos
python main.py
```

**NÃ³i trong khi cháº¡y:**
> "Code Ä‘ang cháº¡y. Em tháº¥y:
> - Loss giáº£m dáº§n tá»« ~0.1 xuá»‘ng ~0.01
> - Network há»c Ä‘Æ°á»£c hÃ m sin(x)
> - CÃ³ so sÃ¡nh giá»¯a Gradient Descent vÃ  BFGS"

**Sau khi cháº¡y xong:**
> "Káº¿t quáº£:
> - Gradient Descent: Loss cuá»‘i = 0.006411 sau 1000 epochs
> - BFGS: Loss cuá»‘i = 0.003510 sau 100 iterations
> - BFGS há»™i tá»¥ nhanh hÆ¡n vÃ  cho loss tháº¥p hÆ¡n"

#### 3.2. Chá»©ng minh gradient Ä‘Ãºng (1-2 phÃºt)

**NÃ³i:**
> "Äá»ƒ chá»©ng minh gradient Ä‘Æ°á»£c tÃ­nh Ä‘Ãºng, em cháº¡y test backpropagation."

**LÃ m:**
```bash
python test_backprop.py
```

**NÃ³i:**
> "Test nÃ y chá»©ng minh:
> - Gradient Ä‘Æ°á»£c tÃ­nh báº±ng Backpropagation tá»± implement
> - Loss giáº£m dáº§n khi train (chá»©ng minh gradient Ä‘Ãºng)
> - KhÃ´ng dÃ¹ng thÆ° viá»‡n AI"

---

### PHáº¦N 4: THIáº¾T Káº¾ ADAPTIVE (2-3 phÃºt)

**NÃ³i:**
> "YÃªu cáº§u thá»© 3 lÃ  thiáº¿t káº¿ adaptive. Em thiáº¿t káº¿ theo hÆ°á»›ng OOP + danh sÃ¡ch layer."

**LÃ m:**
- Má»Ÿ file `network/network.py`
- Chá»‰ vÃ o class `NeuralNetwork`

**Code cáº§n chá»‰:**
```python
class NeuralNetwork:
    def __init__(self, layers):
        self.layers = layers
    
    def forward(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x
    
    def backward(self, grad, lr):
        for layer in reversed(self.layers):
            grad = layer.backward(grad, lr)
        return grad
```

**NÃ³i:**
> "Thiáº¿t káº¿ nÃ y cho phÃ©p thÃªm/bá»›t layer mÃ  khÃ´ng cáº§n sá»­a code training."

**LÃ m:**
- Má»Ÿ file `demos/test_adaptive.py`
- Chá»‰ vÃ o vÃ­ dá»¥

**NÃ³i:**
> "VÃ­ dá»¥: Network Ä‘Æ¡n giáº£n 1->5->1 vÃ  network phá»©c táº¡p 1->10->5->1 Ä‘á»u dÃ¹ng cÃ¹ng má»™t hÃ m train.
> Chá»‰ cáº§n thay Ä‘á»•i khá»Ÿi táº¡o network, code training tá»± Ä‘á»™ng thÃ­ch nghi."

**LÃ m:**
```bash
python test_adaptive.py
```

**NÃ³i:**
> "Test nÃ y chá»©ng minh 3 network khÃ¡c nhau Ä‘á»u train Ä‘Æ°á»£c vá»›i cÃ¹ng má»™t hÃ m, khÃ´ng cáº§n viáº¿t láº¡i code."

---

### PHáº¦N 5: SO SÃNH OPTIMIZER (1-2 phÃºt)

**NÃ³i:**
> "Em Ä‘Ã£ so sÃ¡nh Gradient Descent vÃ  BFGS. Cáº£ hai Ä‘á»u dÃ¹ng gradient tÃ­nh báº±ng Backpropagation tá»± implement."

**LÃ m:**
- Má»Ÿ file `network/optimizers.py`
- Chá»‰ vÃ o `BFGSOptimizer`

**NÃ³i:**
> "QUAN TRá»ŒNG: Trong BFGS, gradient váº«n Ä‘Æ°á»£c tÃ­nh báº±ng Backpropagation tá»± viáº¿t, khÃ´ng dÃ¹ng automatic differentiation cá»§a scipy.
> scipy.optimize.minimize chá»‰ lÃ  wrapper Ä‘á»ƒ tá»‘i Æ°u, gradient Ä‘Æ°á»£c truyá»n vÃ o qua tham sá»‘ jac=gradient."

**Chá»‰ code:**
```python
def gradient(params):
    # Forward pass
    y_pred = network.forward(X[i])
    loss_fn.forward(y_pred, y[i])
    
    # Backward pass - tÃ­nh gradient báº±ng chain rule (tá»± implement)
    grad_loss = loss_fn.backward()
    network.backward(grad_loss, 0)  # Backpropagation tá»± viáº¿t!
    
    return grad_flat
```

---

### PHáº¦N 6: á»¨NG Dá»¤NG (1 phÃºt)

**NÃ³i:**
> "Em Ä‘Ã£ lÃ m 3 á»©ng dá»¥ng:
> 1. Function Approximation: sin(x)
> 2. XOR Problem
> 3. Classification: Circle
> 
> Táº¥t cáº£ Ä‘á»u cháº¡y Ä‘Æ°á»£c vÃ  cho káº¿t quáº£ tá»‘t."

---

### PHáº¦N 7: Káº¾T LUáº¬N (1 phÃºt)

**NÃ³i:**
> "TÃ³m láº¡i, em Ä‘Ã£:
> 1. âœ… TÃ­nh gradient báº±ng Backpropagation tá»± implement, khÃ´ng dÃ¹ng thÆ° viá»‡n AI
> 2. âœ… Code cháº¡y Ä‘Æ°á»£c, cÃ³ káº¿t quáº£ vÃ  visualization
> 3. âœ… Thiáº¿t káº¿ adaptive, thÃªm/bá»›t layer khÃ´ng cáº§n viáº¿t láº¡i code
> 
> Em xin cáº£m Æ¡n tháº§y!"

---

## âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG KHI THUYáº¾T TRÃŒNH

### âœ… NÃŠN LÃ€M:
1. **Chá»‰ code trá»±c tiáº¿p** - Má»Ÿ file vÃ  chá»‰ vÃ o dÃ²ng code cá»¥ thá»ƒ
2. **Nháº¥n máº¡nh "tá»± implement"** - NÃ³i rÃµ khÃ´ng dÃ¹ng automatic differentiation
3. **Cháº¡y code live** - Chá»©ng minh code thá»±c sá»± cháº¡y Ä‘Æ°á»£c
4. **Giáº£i thÃ­ch cÃ´ng thá»©c** - NÃ³i rÃµ gradient Ä‘Æ°á»£c tÃ­nh nhÆ° tháº¿ nÃ o
5. **So sÃ¡nh káº¿t quáº£** - Chá»‰ ra GD vs BFGS

### âŒ KHÃ”NG NÃŠN:
1. âŒ Chá»‰ nÃ³i suÃ´ng, khÃ´ng má»Ÿ code
2. âŒ NÃ³i "dÃ¹ng thÆ° viá»‡n" - Pháº£i nháº¥n máº¡nh "tá»± implement"
3. âŒ Bá» qua pháº§n gradient - ÄÃ¢y lÃ  pháº§n quan trá»ng nháº¥t
4. âŒ KhÃ´ng cháº¡y code - Pháº£i chá»©ng minh code cháº¡y Ä‘Æ°á»£c

---

## ğŸ¯ CÃ‚U TRáº¢ Lá»œI CHO CÃC CÃ‚U Há»I THÆ¯á»œNG Gáº¶P

### Q: "Gradient Ä‘Æ°á»£c tÃ­nh nhÆ° tháº¿ nÃ o?"
**A:** "Em tÃ­nh gradient báº±ng Chain Rule. Vá»›i má»—i layer, em tÃ­nh:
- grad_W = Î´ @ a^T (gradient cá»§a trá»ng sá»‘)
- grad_b = Î´ (gradient cá»§a bias)
- grad_x = W^T @ Î´ (gradient truyá»n vá» layer trÆ°á»›c)
Táº¥t cáº£ Ä‘á»u theo cÃ´ng thá»©c toÃ¡n há»c, em tá»± implement trong hÃ m backward() cá»§a DenseLayer."

### Q: "CÃ³ dÃ¹ng automatic differentiation khÃ´ng?"
**A:** "KhÃ´ng áº¡. Em tÃ­nh gradient hoÃ n toÃ n báº±ng cÃ´ng thá»©c toÃ¡n há»c. Em khÃ´ng dÃ¹ng PyTorch's autograd hay TensorFlow's gradient tape. Em tá»± viáº¿t cÃ´ng thá»©c Ä‘áº¡o hÃ m trong code."

### Q: "LÃ m sao chá»©ng minh gradient Ä‘Ãºng?"
**A:** "Em chá»©ng minh báº±ng cÃ¡ch:
1. Loss giáº£m dáº§n khi train (náº¿u gradient sai, loss sáº½ khÃ´ng giáº£m)
2. Network há»c Ä‘Æ°á»£c pattern tá»« dá»¯ liá»‡u
3. CÃ³ test file test_backprop.py chá»©ng minh gradient Ä‘Æ°á»£c tÃ­nh"

### Q: "Thiáº¿t káº¿ adaptive lÃ  gÃ¬?"
**A:** "Thiáº¿t káº¿ adaptive nghÄ©a lÃ  khi thÃªm/bá»›t layer hoáº·c thay Ä‘á»•i sá»‘ neuron, code training khÃ´ng cáº§n viáº¿t láº¡i. Em dÃ¹ng OOP + danh sÃ¡ch layer, má»—i layer tá»± quáº£n lÃ½ forward vÃ  backward. CÃ³ file test_adaptive.py chá»©ng minh Ä‘iá»u nÃ y."

### Q: "BFGS cÃ³ dÃ¹ng gradient tá»± tÃ­nh khÃ´ng?"
**A:** "CÃ³ áº¡. Trong BFGS, em váº«n tÃ­nh gradient báº±ng Backpropagation tá»± implement. scipy.optimize.minimize chá»‰ lÃ  wrapper Ä‘á»ƒ tá»‘i Æ°u, gradient Ä‘Æ°á»£c truyá»n vÃ o qua tham sá»‘ jac=gradient. Em cÃ³ thá»ƒ chá»‰ code trong BFGSOptimizer Ä‘á»ƒ chá»©ng minh."

---

## ğŸ“ CHECKLIST TRÆ¯á»šC KHI THUYáº¾T TRÃŒNH

- [ ] ÄÃ£ test code cháº¡y Ä‘Æ°á»£c
- [ ] ÄÃ£ chuáº©n bá»‹ terminal sáºµn sÃ ng
- [ ] ÄÃ£ má»Ÿ sáºµn cÃ¡c file code quan trá»ng (layers.py, loss.py, network.py)
- [ ] ÄÃ£ chuáº©n bá»‹ cÃ¢u tráº£ lá»i cho cÃ¡c cÃ¢u há»i thÆ°á»ng gáº·p
- [ ] ÄÃ£ nhá»› cÃ¡c Ä‘iá»ƒm quan trá»ng cáº§n nháº¥n máº¡nh
- [ ] ÄÃ£ test thá»i gian thuyáº¿t trÃ¬nh (10-15 phÃºt)

---

## ğŸš€ Lá»†NH Cáº¦N CHUáº¨N Bá»Š

```bash
# Terminal 1: Cháº¡y demo chÃ­nh
cd /home/thuypm/Desktop/ttu/pptinh/final/demos
python main.py

# Terminal 2: Test backprop
cd /home/thuypm/Desktop/ttu/pptinh/final/demos
python test_backprop.py

# Terminal 3: Test adaptive
cd /home/thuypm/Desktop/ttu/pptinh/final/demos
python test_adaptive.py
```

---

## ğŸ’¡ TIPS THUYáº¾T TRÃŒNH

1. **Tá»± tin** - Báº¡n Ä‘Ã£ lÃ m Ä‘Ãºng, code cháº¡y Ä‘Æ°á»£c
2. **Chá»‰ code cá»¥ thá»ƒ** - Äá»«ng nÃ³i chung chung, chá»‰ vÃ o dÃ²ng code
3. **Nháº¥n máº¡nh "tá»± implement"** - ÄÃ¢y lÃ  Ä‘iá»ƒm máº¡nh cá»§a báº¡n
4. **Cháº¡y code live** - Chá»©ng minh code thá»±c sá»± hoáº¡t Ä‘á»™ng
5. **Giáº£i thÃ­ch rÃµ rÃ ng** - NÃ³i cháº­m, rÃµ rÃ ng, dá»… hiá»ƒu

---

**CHÃšC Báº N THUYáº¾T TRÃŒNH THÃ€NH CÃ”NG! ğŸ‰**

